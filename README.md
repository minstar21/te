# Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis
Repository of paper "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis" (ACL 2025 Main)
Paper reproduce
--> https://github.com/GaryStack/Trustworthy-Evaluation

## ğŸš€ Getting Start
0. Comparative Analysis, Causal Analysis êµ¬í˜„ì™„ë£Œ
1. 00_1score_pipecs.py: íŒ¨ì¹­ ì‹œ ì»¤ì¥¬ì–¼ ìŠ¤ì½”ì–´ë§Œ ì‚¬ìš©
2. 01_change_scores.py: íŒ¨ì¹­ ì‹œ Comparative, Causal ë¶„ì„ ê²°ê³¼ ë‰´ëŸ° ê°’ì„ ë‹¨ìˆœí•©í•˜ì—¬ íŒ¨ì¹­
ê·¸ ì™¸ ë¯¸ì™„ì„±.
```shell
python -m 00_1score_pipecs \
  --dataset /data/TOFU_forget10_train.csv \
  --save_dir ./outputs/llama3_eval \
  --output_file ./outputs/llama3_eval/final_scores.pt \
  --tokenizer_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --base_model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --red_model_name_or_path /nas/home/mhlee/open-unlearning/saves/finetune/tofu_Llama-3.2-1B-Instruct_retain90 \
  --blue_model_name_or_path /nas/home/mhlee/open-unlearning/saves/unlearn/tofu_Llama-3.2-1B-Instruct_forget10_GradAscent\
  --patch_block 512 \
  --eval_batch_size 12 \
  --max_num_examples 400 \
  --use_chat_format \
  --use_slow_tokenizer
```

## ğŸš€ Experiments protocol
0. Tofu forget 10 ë°ì´í„°ë¥¼ í™œìš©í•œ íŒŒì¸íŠœë‹/ì–¸ëŸ¬ë‹ ëª¨ë¸ ìƒì„±(forget data 400)
1. Base ëª¨ë¸(íŒŒì¸íŠœë‹ì„ ì•„ì˜ˆ í•˜ì§€ ì•Šì€) ê¸°ì¤€ ì„ ë³„ëœ ë‰´ëŸ°ì— íŒ¨ì¹­
2. fine tuned(retain only) ëª¨ë¸(ì˜¤ì—¼) Unlearning(GA ê¸°ë°˜ êº ë—í•œ) ëª¨ë¸ì— íŒ¨ì¹­ ì‹œ TOFU ë°ì´í„°ì…‹ RougeL score ë¹„êµ 
